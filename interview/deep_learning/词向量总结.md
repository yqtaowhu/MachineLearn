- [词向量总结](#%e8%af%8d%e5%90%91%e9%87%8f%e6%80%bb%e7%bb%93)
  - [skip-gram和cbow的区别](#skip-gram%e5%92%8ccbow%e7%9a%84%e5%8c%ba%e5%88%ab)
  - [Transformers](#transformers)
  - [BERT／GPT／Elmo的区别？BERT的优点在哪？](#bert%ef%bc%8fgpt%ef%bc%8felmo%e7%9a%84%e5%8c%ba%e5%88%abbert%e7%9a%84%e4%bc%98%e7%82%b9%e5%9c%a8%e5%93%aa)
  - [参考资料](#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99)


# 词向量总结

## skip-gram和cbow的区别


## Transformers
seq2seq model with self-attention


## BERT／GPT／Elmo的区别？BERT的优点在哪？

1. elmo 

- ELMo主要是用来做特征抽取，应用到下游任务的时候，模型参数不更新
- 架构上是利用两个双向LSTM，一个对输入句子进行正向编码，一个对输入句子进行反向编码，然后将两个编码连接在一起，作为下游任务的输入。

2. GPT
 
- GPT可以用来微调，应用到下游任务的时候，可以根据具体任务，更新模型参数
- 架构上是堆叠的单向Transformer，可以看作是Encoder-Decoder架构的Encoder部分，但是预训练的时候为了避免self-attention看到整句的信息，借鉴了Decoder的Mask方法，就是训练到时刻t的时候，将t+1以及之后的信息屏蔽掉，这也就是单向的原因。

3. bert: Bidirectional Encoder Representation from Transformers
- 双向的transfermer作为特征抽取器(双向语言模型)
- Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。



ELMo虽然是双向的，但是无法将上下文编码在同一个向量里，比如

“这个小狗无法跳上台阶因为它太累了”或者“这个小狗无法跳上台阶因为它太高了”

正向编码无法得知“它”是谁，因为看不到后边的“累”或者“高”

反向编码无法得知“它”是谁，因为看不到前边的“小狗”或者“台阶”

GPT中的Self-Attention本身是可以同时利用“它”前边和后边的信息，但是为了防止看到未来的信息，引入了Mask机制，所以其实看不到后边的信息。

GPT采用的是普通的语言模型的策略，也就是根据历史信息预测当前信息。BERT为了解决GPT中看不到未来信息，只能利用一个方向的信息的问题，引入了Masked LM。类似完形填空，训练的时候，遮蔽一些词，然后让BERT预测这些词，目标函数就是使得预测的词尽可能正确。

强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。

OpenAI GPT使用的是BooksCorpus语料，总的词数800M；而BERT还增加了wiki语料，其词数是2,500M。所以BERT训练数据的总词数是3,300M。


## 参考资料

- [从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
- 